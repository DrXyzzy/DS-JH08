---
title: "Prediction Assignment Writeup"
author: "Hal Snyder"
date: "December 23, 2016"
output:
  html_document:
    keep_md: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Six participants fitted with wearable sensors performed 10 sets of bicep curls in five different fashions (exercise classes `A` through `E`). The goal of this project is to construct and evaluate a machine learning model to predict which exercise classes was performed for given observations, using sensor readings as predictors.

Reference: [Human Activity Recognition, Weight-Lifting Exercises](http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises#ixzz4TjKsbTNc):  
Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

## Setup and Input

Libraries

```{r libs}
library(caret)
```

Load Data

```{r input}
training <- read.csv("pml-training.csv",na.strings=c("NA","","#DIV/0!"))
dim(training)
testing <- read.csv("pml-testing.csv",na.strings=c("NA","","#DIV/0!"))
dim(testing)
```

## Tidy the Data

There are 60 variables in the training set with no missing values. The other 100 variables each have more than 19000 missing values; drop these from training and test sets.

```{r drop missing, fig.cap="number of NA values vs variable index"}
na.counts <- colSums(is.na(training))
table(na.counts)
hist(na.counts)
training.no.na <- training[na.counts == 0]
dim(training.no.na)
testing.no.na <- testing[na.counts == 0]
dim(testing.no.na)
```

Drop more variables

- drop column 1, it is row number in the csv file, no predictive value
- drop timestamps (columns 3-5), see if we can get by without using them
- drop new_window (column 6) logical variable which is mostly `no` in training

```{r drop more}
table(training$new_window)
train.noX <- training.no.na[c(-1, -3, -4, -5, -6)]
dim(train.noX)
test.noX <- testing.no.na[c(-1, -3, -4, -5, -6)]
dim(test.noX)
```
## Explore and Preprocess Data

Variable `user_name` (column 1) appears to be significant. (Note that taking `user_name` as predictor implies the model will require calibrating to each new user.)

```{r user_name, fig.cap="classe vs user_name"}
table(training$user_name)
plot(training[,c("user_name","classe")])
```

Convert to `user_name` to 6 numeric variables (dummy variables) to allow `gbm` modeling.

```{r}
dv <- dummyVars(~ user_name,data=test.noX)
train.dv <- cbind(predict(dv,newdata=train.noX),train.noX[,-1])
dim(train.dv)
test.dv <- cbind(predict(dv,newdata=test.noX),test.noX[,-1])
dim(test.dv)
```
Check for near-zero covariance to see if we can drop more variables. No, there are not any variables with zero variance or near-zero variance.
```{r}
nzc <- nearZeroVar(train.dv, saveMetrics = T)
table(nzc$nzv)
table(nzc$zeroVar)
```
Compute principal component analysis to see if some of the variables can be dropped. Gradual fall-off of curve suggests we can't reduce number of variables significantly by dropping low-importance variables, so won't use PCA for the model.
```{r pca, fig.cap="PCA variable importance as indicated by std dev"}
p0 <- prcomp(subset(train.dv, select = -c(classe)))
plot(p0$sdev)
```

Subset the training set to reduce model run time. Use 1/10 of original training set.
```{r subset}
set.seed(107)
inTr <- createDataPartition(y=train.dv$classe,p=0.1,list=F)
train.dv.subset <- train.dv[inTr,]
dim(train.dv.subset)
```

## Buid and Evaluate the Model

Each section addresses one of the 4 parts of the project assignment.

### 1. How the Model is Built

Because outcome is a nominal variable, use gradient-boosting multinomial logistic regression (`gbm`) with k-fold cross-validation. Report on run-time of the model on the training subset used.

```{r build, cache=TRUE}
t1 <- Sys.time()
train_control<- trainControl(method="cv", number=10, savePredictions = TRUE)
mod1 <- train(classe ~ ., data=train.dv.subset, trControl=train_control, method="gbm", verbose=F)
Sys.time() - t1
```
Compute confusion matrix for training subset.

```{r confuse0}
confusionMatrix.train(mod1)
```
Compute confusion matrix obtained by applying model to entire training set.
```{r confuse`}
confusionMatrix(train.dv$classe,predict(mod1,newdata=train.dv))$table
confusionMatrix(train.dv$classe,predict(mod1,newdata=train.dv))$overall[1]
```

### 2. How Cross-Validation Is Used

K-fold cross-validation is used with k = 10.

### 3. Expected Out-of-Sample Error

Because 9/10 of training set was not used to build the model, use it to estimate out-of-sample error.
```{r oose}
train.oos.subset <- train.dv[-inTr,]
dim(train.oos.subset)
cm.oos <- confusionMatrix(train.oos.subset$classe,
                          predict(mod1,newdata=train.oos.subset))
cm.oos$overall[1]
print(paste('expected out of sample error', as.numeric(1 - cm.oos$overall[1])))

```

### 4. Why I Made the Choices I Did

- A subset of the initial training set was used to build the model because
    - build time with the full training set was > 20 minutes
    - 10% subset gave sufficient accuracy
- Gradient-boosting multinomial logistic regression (`gbm`) was used because outcome is a nominal variable and most predictors take on continuous values.
- K-fold cross-validation with value of 10 for k because that value is efficient for 60 predictors; leave-one-out cross-validation would have been too slow.

## Predict Test Set Outcomes

Here are the predictions provided by the above model for the 20 observations in the test set.
```{r predict20}
predict(mod1,newdata=test.dv)
```

## Conclusion

Expected accuracy near 95% is obtained modeling the given data using gradient-boosted multinomial logistic regression with 10-fold cross-validation.
